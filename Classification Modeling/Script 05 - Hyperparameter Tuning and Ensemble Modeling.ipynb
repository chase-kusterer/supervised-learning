{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<br><h2>Script 05 | Hyperparameter Tuning and Ensemble Modeling</h2>\n",
    "<br>\n",
    "Written by Chase Kusterer<br>\n",
    "<a href=\"https://github.com/chase-kusterer\">GitHub</a> | <a href=\"https://www.linkedin.com/in/kusterer/\">LinkedIn</a>\n",
    "<br><br><br>\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<h2>Part I: Preparation</h2>\n",
    "<br>\n",
    "Run the following code to import necessary packages, load data, and set display options for pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# importing packages\n",
    "########################################\n",
    "\n",
    "# essentials\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import pandas            as pd  # data science essentials\n",
    "import numpy             as np  # mathematical essentials\n",
    "import warnings\n",
    "\n",
    "\n",
    "# model preparation\n",
    "from sklearn.preprocessing import StandardScaler       # standard scaler\n",
    "from sklearn.model_selection import train_test_split   # train-test split\n",
    "from sklearn.model_selection import RandomizedSearchCV # hp tuning\n",
    "\n",
    "\n",
    "# model results\n",
    "from sklearn.metrics import roc_auc_score              # auc score\n",
    "from sklearn.metrics import make_scorer                # customizable scorer\n",
    "from sklearn.metrics import confusion_matrix           # confusion matrix\n",
    "\n",
    "\n",
    "# machine learning\n",
    "from sklearn.tree import DecisionTreeClassifier         # classification trees\n",
    "from sklearn.tree import plot_tree                      # tree plots\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "\n",
    "\n",
    "########################################\n",
    "# loading data and setting display options\n",
    "########################################\n",
    "# loading data\n",
    "titanic = pd.read_excel(io = './datasets/titanic_feature_rich.xlsx')\n",
    "\n",
    "\n",
    "## Options ##\n",
    "# setting pandas print options and supressing warnings\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "warnings.simplefilter(action = 'ignore', category = UserWarning)\n",
    "\n",
    "\n",
    "## this code will not produce an output ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>User-Defined Functions</strong><br>\n",
    "Run the following code to load the user-defined functions used throughout this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     35,
     203,
     229
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "## tuning_results ##\n",
    "####################\n",
    "def tuning_results(cv_results, n=5):\n",
    "    \"\"\"\n",
    "    This function will display the top \"n\" models from hyperparameter tuning,\n",
    "    based on \"rank_test_score\".\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    cv_results = results dictionary from the attribute \".cv_results_\"\n",
    "    n          = number of models to display\n",
    "    \"\"\"\n",
    "    param_lst = []\n",
    "\n",
    "    for result in cv_results[\"params\"]:\n",
    "        result = str(result).replace(\":\", \"=\")\n",
    "        param_lst.append(result[1:-1])\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(data = {\n",
    "        \"Model_Rank\" : cv_results[\"rank_test_score\"],\n",
    "        \"Mean_Test_Score\" : cv_results[\"mean_test_score\"],\n",
    "        \"SD_Test_Score\" : cv_results[\"std_test_score\"],\n",
    "        \"Parameters\" : param_lst\n",
    "    })\n",
    "\n",
    "\n",
    "    results_df = results_df.sort_values(by = \"Model_Rank\", axis = 0)\n",
    "    return results_df.head(n = n)\n",
    "\n",
    "\n",
    "#####################\n",
    "## sklearn_summary ##\n",
    "#####################\n",
    "def classification_summary(x,\n",
    "                           y,\n",
    "                           model,\n",
    "                           model_name   = \"\",\n",
    "                           results_df   = None,\n",
    "                           tt_split     = True,\n",
    "                           test_size    = 0.25,\n",
    "                           scale        = False,\n",
    "                           full_tree    = False,\n",
    "                           random_state = 702):\n",
    "    \"\"\"  \n",
    "    This function is designed to generate summary statistics for the following\n",
    "    classification models from scikit-learn:\n",
    "    * LogisticRegression         - Logistic Regression\n",
    "    * DecisionTreeClassifier     - Classification Tree\n",
    "    * RandomForestClassifier     - Random Forest\n",
    "    * GradientBoostingClassifier - Gradient Boosted Machine\n",
    "\n",
    "\n",
    "    Additional Functionality\n",
    "    ------------------------\n",
    "    This function will standardize the data using StandardScaler() and create\n",
    "    training and testing sets using train-test split, stratifying the\n",
    "    y-variable.\n",
    "    \n",
    "    It will also output a tabular confusion matrix, calculate area under the\n",
    "    ROC curve (AUC) for the training and testing sets, as well as the train-\n",
    "    test gap.\n",
    "    \n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    x            | array     | X-data before train-test split | No default.\n",
    "    y            | array     | y-data before train-test split | No default.\n",
    "    model        | model     | model object to instantiate    | No default.\n",
    "    model_name   | str       | option to name the model       | Default = \"\"\n",
    "    results_df   | DataFrame | place to store model results   | Default = None\n",
    "    test_size    | float     | test set proportion            | Default = 0.25\n",
    "    scale        | bool      | whether to scale the data      | Default = False\n",
    "    random_state | int       | seed for train-test split      | Default = 702\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########\n",
    "    # scaling #\n",
    "    ###########\n",
    "    \n",
    "    if scale == True:\n",
    "        # instantiating a StandardScaler() object\n",
    "        scaler = StandardScaler(copy = True)\n",
    "\n",
    "\n",
    "        # FITTING the scaler with the data\n",
    "        scaler.fit(x)\n",
    "\n",
    "        # TRANSFORMING our data after fit\n",
    "        x_scaled = scaler.transform(x)\n",
    "\n",
    "        # converting scaled data into a DataFrame\n",
    "        x_scaled_df = pd.DataFrame(x_scaled)\n",
    "\n",
    "        # reattaching column names\n",
    "        x_scaled_df.columns = list(x.columns)\n",
    "\n",
    "        # reverting back to x as the DataFrame's name\n",
    "        x = x_scaled_df\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    # train-test split #\n",
    "    ####################\n",
    "    # standard train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, # x\n",
    "                                                        y, # y\n",
    "                                                        test_size    = test_size,\n",
    "                                                        random_state = random_state,\n",
    "                                                        stratify     = y)\n",
    "    \n",
    "    \n",
    "    #########################\n",
    "    # fit - predict - score #\n",
    "    #########################\n",
    "    # fitting to training data\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # predicting on new data\n",
    "    model_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # scoring results\n",
    "    model_train_auc   = round(roc_auc_score(y_true  = y_train,\n",
    "                              y_score = model.predict(x_train)), ndigits = 4) # auc\n",
    "    \n",
    "    model_test_auc    = round(roc_auc_score(y_true  = y_test,\n",
    "                              y_score = model.predict(x_test)),  ndigits = 4) # auc\n",
    "\n",
    "    model_gap         = round(abs(model_train_auc - model_test_auc), ndigits = 4)\n",
    "\n",
    "    \n",
    "    ####################\n",
    "    # confusion matrix #\n",
    "    ####################\n",
    "    full_tree_tn, \\\n",
    "    full_tree_fp, \\\n",
    "    full_tree_fn, \\\n",
    "    full_tree_tp = confusion_matrix(y_true = y_test, y_pred = model_pred).ravel()\n",
    "\n",
    "    \n",
    "    ###########################\n",
    "    # storing/showing results #\n",
    "    ###########################\n",
    "    # instantiating a list to store model results\n",
    "    results_lst = [ model_name, model_train_auc, model_test_auc, model_gap ]\n",
    "\n",
    "    # converting to DataFrame\n",
    "    results_lst = pd.DataFrame(data = results_lst)\n",
    "\n",
    "    # transposing (rotating) DataFrame\n",
    "    results_lst = np.transpose(a = results_lst)\n",
    "    \n",
    "    # if no results DataFrame provided\n",
    "    if results_df == None:\n",
    "\n",
    "        # concatenating to coef_df\n",
    "        results_df = pd.DataFrame(data = results_lst)\n",
    "    \n",
    "    # if results DataFrame provided\n",
    "    else:\n",
    "        \n",
    "        # concatenating to coef_df\n",
    "        results_df = pd.concat(objs = [results_df, results_lst],\n",
    "                               axis         = 0,\n",
    "                               ignore_index = True)\n",
    "        \n",
    "    # adding column names\n",
    "    results_columns = ['Model Name', 'train_auc', 'test_auc', 'tt_gap']\n",
    "    \n",
    "    # renaming columns\n",
    "    results_df.columns = results_columns\n",
    "    \n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Results for {model_name}\n",
    "    {'=' * 20}\n",
    "    Model Type: {model}\n",
    "    Training Samples: {len(x_train)} \n",
    "    Testing  Samples: {len(x_test)}\n",
    "    \n",
    "    \n",
    "    Summary Statistics\n",
    "    ------------------\n",
    "    AUC (Train): {model_train_auc}\n",
    "    AUC (Test) : {model_test_auc}\n",
    "    TT Gap     : {model_gap}\n",
    "    \n",
    "    \n",
    "    Confusion Matrix (test set)\n",
    "    ---------------------------\n",
    "    True Negatives : {full_tree_tn}\n",
    "    False Positives: {full_tree_fp}\n",
    "    False Negatives: {full_tree_fn}\n",
    "    True Positives : {full_tree_tp}\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    labels : DataFrame with labels (i.e., x_data)\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = x_data.shape[1]\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Feature_Importance_Plot.png')\n",
    "        \n",
    "        \n",
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "    Creates a visualization of a confusion matrix.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    true_y : true values for the response variable\n",
    "    pred_y : predicted values for the response variable\n",
    "    labels : , default None\n",
    "        \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>Preparing Data</strong><br>\n",
    "Run the following code cells to prepare the x- and y-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# reversing m_boat\n",
    "titanic['lifeboat'] = abs(titanic['m_boat'] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# preparing to partition data\n",
    "x_data   =  titanic.drop(['survived', 'm_boat', 'lifeboat',\n",
    "                          'male', 'pclass_3'],\n",
    "                               axis = 1)\n",
    "\n",
    "\n",
    "y_data =  titanic['lifeboat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part II: Hyperparameter Tuning</h2>\n",
    "<br>\n",
    "<strong>The Analytics Kitchen</strong><br>\n",
    "Model selection can be thought of as selecting from the various appliances that can be used for cooking. Hyperparameter tuning can be thought of as an extension of this. For example, if we wanted to cook something in the oven, how hot should the oven be in order to get the best results? How does this compare to using a microwave given its best settings for the job (time, wattage, etc.)?<br><br>\n",
    "In the same way that we might adjust the temperature of an oven, we can make adjustments to the <strong>hyperparameters</strong> of a machine learning algorithm in order to optimize its results. <a href = \"https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)\">This Wikipedia page</a> does an excellent job of defining a hyperparameter as: <em>a parameter whose value is set before the learning process begins</em>. In other words, these are arguments that are set before a model sees any data. Available hyperparameters can be found in a model's documentation as optional arguments.\n",
    "<br><br>\n",
    "<strong>Cross-Validated Randomized Search</strong><br>\n",
    "We could manually analyze each combination of hyperparameter values one by one, but this would take a very long time. Instead, we can automate this process by using <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">RandomizedSearchCV</a> from scikit-learn. Note that there is a similar method to RandomizedSearchCV called GridSearchCV. GridSearchCV performs an exhaustive search, meaning it will try every combination of hyperparameters it is given. Try out this method if you dare, but keep in mind that it is notoriously slow and may not lead to better results than RandomizedSearchCV.<br><br>\n",
    "<strong>Note:</strong> Make sure your hyperparameter ranges are of a reasonable size to mitigate a processing bottleneck.\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>a) Use the documentation below to discover the hyperparameters for classification trees.</h4>\n",
    "Then, complete the code to tune the listed hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# developing hyperparameter ranges\n",
    "criterion_range = [\"gini\", \"entropy\", \"log_loss\"] # criterion\n",
    "#splitter_range  = _____ # splitter\n",
    "#depth_range     = _____ # max_depth\n",
    "#leaf_range      = _____ # min_samples_leaf\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'criterion' : criterion_range,}\n",
    "              #'NAME OF HYPERPARAMETER' : HYPERPARAMETER_RANGE,\n",
    "              #'NAME OF HYPERPARAMETER' : HYPERPARAMETER_RANGE,\n",
    "              #'NAME OF HYPERPARAMETER' : HYPERPARAMETER_RANGE}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "model = DecisionTreeClassifier(random_state = 708)\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "tuned_model = RandomizedSearchCV(estimator             = model,\n",
    "                                 param_distributions   = param_grid,\n",
    "                                 cv                    = 5,\n",
    "                                 n_iter                = 1000,\n",
    "                                 random_state          = 702,\n",
    "                                 scoring             = make_scorer(roc_auc_score,\n",
    "                                                                   needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_model.fit(x_data, y_data)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_model.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_model.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# developing hyperparameter ranges\n",
    "criterion_range = [\"gini\", \"entropy\", \"log_loss\"] # criterion\n",
    "splitter_range  = ['best', 'random']              # splitter\n",
    "depth_range     = np.arange(1, 11, 1)             # max_depth\n",
    "leaf_range      = np.arange(1, 1001, 1)           # min_samples_leaf\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'criterion'        : criterion_range,\n",
    "              'splitter'         : splitter_range,\n",
    "              'max_depth'        : depth_range,\n",
    "              'min_samples_leaf' : leaf_range}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "model = DecisionTreeClassifier(random_state = 708)\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "tuned_model = RandomizedSearchCV(estimator             = model,\n",
    "                                 param_distributions   = param_grid,\n",
    "                                 cv                    = 5,\n",
    "                                 n_iter                = 1000,\n",
    "                                 random_state          = 702,\n",
    "                                 scoring             = make_scorer(roc_auc_score,\n",
    "                                                                   needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_model.fit(x_data, y_data)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_model.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_model.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>b) Build a classification tree based on the hyperparameter tuning results.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# instantiating a classification tree\n",
    "model = DecisionTreeClassifier(splitter         = _____,\n",
    "                                    min_samples_leaf = _____,\n",
    "                                    max_depth        = _____,\n",
    "                                    criterion        = _____)\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = _____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# instantiating a classification tree\n",
    "model = DecisionTreeClassifier(splitter         = 'best',\n",
    "                               min_samples_leaf = 10,\n",
    "                               max_depth        = 9,\n",
    "                               criterion        = 'entropy')\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = \"Tuned Classification Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>c) (Optional) Plot the tree graphically.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# setting figure size\n",
    "plt.figure(figsize=(48, 12))\n",
    "\n",
    "\n",
    "# developing a plotted tree\n",
    "plot_tree(decision_tree = _____, \n",
    "          feature_names = _____,\n",
    "          filled        = True, \n",
    "          rounded       = True, \n",
    "          fontsize      = 12)\n",
    "\n",
    "\n",
    "# rendering the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# setting figure size\n",
    "plt.figure(figsize=(48, 12))\n",
    "\n",
    "\n",
    "# developing a plotted tree\n",
    "plot_tree(decision_tree = model, \n",
    "          feature_names = x_data.columns,\n",
    "          filled        = True, \n",
    "          rounded       = True, \n",
    "          fontsize      = 12)\n",
    "\n",
    "\n",
    "# rendering the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part III: Analyzing Hyperparameter Results</h2><br>\n",
    "The following codes will help in analyzing the results of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "tuned_model.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# checking documentation\n",
    "help(tuning_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# run tuning_results() on the hyperparameter tuning results\n",
    "tuning_results(cv_results = tuned_model.cv_results_, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part IV: Random Forest</h2>\n",
    "\n",
    "A random forest can be thought of as a group of decision trees that are all slightly different from each other. This model type starts by randomly selecting a subset of x-features and builds the best decision tree it can given this information. Afterwards, the model randomly selects a different set of x-features and builds another tree. By building several trees, each observation has been predicted several times. Think of this as each tree getting a vote on whether an observation should be a zero or a one (majority wins). After all votes have been cast, whichever class has the most votes wins and prediction on that observation is complete. For example, if an observation was predicted as zero 20% of the time and one 80% of the time, the final prediction for this observation will be one.<br><br>\n",
    "<h4>a) Build a random forest.</h4>\n",
    "Build a random forest using the default values for the hyperparameters listed below. Remember, default values are documented in help(&nbsp;) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a random forest model with default hyperparameters\n",
    "model = RandomForestClassifier(n_estimators     = _____,\n",
    "                               criterion        = _____,\n",
    "                               max_depth        = _____,\n",
    "                               min_samples_leaf = _____,\n",
    "                               bootstrap        = _____,\n",
    "                               warm_start       = _____,\n",
    "                               random_state     = 702)\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = \"Default Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a random forest model with default hyperparameters\n",
    "model = RandomForestClassifier(n_estimators     = 100,\n",
    "                               criterion        = 'gini',\n",
    "                               max_depth        = None,\n",
    "                               min_samples_leaf = 1,\n",
    "                               bootstrap        = True,\n",
    "                               warm_start       = False,\n",
    "                               random_state     = 702)\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = \"Default Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>b) Write and run the <em>plot_feature_importances</em> function in the code cell below.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# plotting feature importances\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# plotting feature importances\n",
    "plot_feature_importances(model  = model,\n",
    "                         train  = x_data,\n",
    "                         export = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "Run the following code to tune random forest hyperparameters using <strong>RandomizedSearchCV</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# instantiating a hyperparameter space\n",
    "estimator_range  = np.arange(100, 1501, 100)\n",
    "leaf_range       = np.arange(1, 31, 10)\n",
    "criterion_range  = ['gini', 'entropy']\n",
    "bootstrap_range  = [True, False]\n",
    "warm_start_range = [True, False]\n",
    "max_depth        = np.arange(1, 11, 1)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'n_estimators'     : estimator_range,\n",
    "              'min_samples_leaf' : leaf_range,\n",
    "              'criterion'        : criterion_range,\n",
    "              'bootstrap'        : bootstrap_range,\n",
    "              'warm_start'       : warm_start_range}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "model = RandomForestClassifier(random_state = 702)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "tuned_model = RandomizedSearchCV(estimator           = model,\n",
    "                                 param_distributions = param_grid,\n",
    "                                 cv                  = 5,\n",
    "                                 n_iter              = 1000,\n",
    "                                 random_state        = 702,\n",
    "                                 scoring             = make_scorer(roc_auc_score,\n",
    "                                                                   needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_model.fit(x_data, y_data)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_model.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_model.best_score_.round(decimals = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# best estimators based on RandomizedSearchCV\n",
    "tuned_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>Improving Processing Efficiency</strong><br>\n",
    "Automated hyperparameter optimization can take a long time. In order to avoid having to run this each time a script is utilized, it is a good practice to:\n",
    "* Run an automated hyperparameter optimization technique and record its results\n",
    "* Comment out (but not delete) the hyperparameter optimization code\n",
    "* Manually set each hyperparameter when building a tuned model\n",
    "\n",
    "<br>\n",
    "This will help alleviate the processing bottleneck while allowing you to uncomment and rerun the optimization code if needed.\n",
    "<h4>c) Complete the code to create a tuned random forest model.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a random forest\n",
    "model = _____\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = \"Tuned Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a random forest\n",
    "model = RandomForestClassifier(criterion        = 'entropy',\n",
    "                               min_samples_leaf = 11,\n",
    "                               n_estimators     = 100,\n",
    "                               warm_start       = True,\n",
    "                               bootstrap        = False,\n",
    "                               random_state     = 702)\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = \"Tuned Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "Run the following code and analyze feature importance of the tuned random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# plotting feature importances\n",
    "plot_feature_importances(model  = model ,\n",
    "                         train  = x_data,\n",
    "                         export = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part V: Gradient Boosted Machines</h2>\n",
    "\n",
    "Gradient boosted machines (GBMs) are like random forests, but instead of starting fresh with each iteration, they learn from the results of trees that have already been built. GBMs also use row-wise optimization instead of a column-wise optimization. This is a similar concept to regularization (ridge, lasso, etc.), but is focused on observations instead of coefficients.<br><br>\n",
    "\n",
    "<h4>a) Develop a gradient boosting classifier model.</h4>\n",
    "Develop a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\">GradientBoostingClassifier</a> model with default values for the hyperparameters listed below. Remember, default values are documented in the help(&nbsp;) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "model = GradientBoostingClassifier(loss          = _____,\n",
    "                                   learning_rate = _____,\n",
    "                                   n_estimators  = _____,\n",
    "                                   criterion     = _____,\n",
    "                                   max_depth     = _____,\n",
    "                                   warm_start    = _____,\n",
    "                                   random_state  = 702)\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = \"Default Gradient Boosted Machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "model = GradientBoostingClassifier(loss          = 'log_loss',\n",
    "                                   learning_rate = 0.1,\n",
    "                                   n_estimators  = 100,\n",
    "                                   criterion     = 'friedman_mse',\n",
    "                                   max_depth     = 3,\n",
    "                                   warm_start    = False,\n",
    "                                   random_state  = 702)\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = \"Default Gradient Boosted Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "Notice that we are using <em>friedman_mse</em> as the criterion above. Friedman proposed that instead of focusing on one MSE value for an entire tree, the algorithm should localize and converge on an optimal MSE for each region of the tree.\n",
    "<br>\n",
    "<h4>c) Complete the code to optimize the hyperparameters for the GBM model.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# instantiating a hyperparameter ranges\n",
    "loss_range       = _____\n",
    "learn_range      = _____\n",
    "estimator_range  = _____\n",
    "criterion_range  = _____\n",
    "depth_range      = _____\n",
    "warm_start_range = _____\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {_____}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "model = GradientBoostingClassifier(random_state = 702)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "tuned_model = RandomizedSearchCV(estimator           = model,\n",
    "                                 param_distributions = param_grid,\n",
    "                                 cv                  = 5,\n",
    "                                 n_iter              = 500,\n",
    "                                 random_state        = 702,\n",
    "                                 scoring             = make_scorer(roc_auc_score,\n",
    "                                                                   needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_model.fit(x_data, y_data)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_model.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_model.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# instantiating a hyperparameter ranges\n",
    "loss_range       = ['log_loss', 'exponential']\n",
    "learn_range      = np.arange(0.1, 2.2, 0.5)\n",
    "estimator_range  = np.arange(100, 1501, 100)\n",
    "criterion_range  = ['friedman_mse', 'squared_error']\n",
    "depth_range      = np.arange(2, 11, 2)\n",
    "warm_start_range = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'loss'          : loss_range,\n",
    "              'learning_rate' : learn_range,\n",
    "              'max_depth'     : depth_range,\n",
    "              'criterion'     : criterion_range,\n",
    "              'n_estimators'  : estimator_range,\n",
    "              'warm_start'    : warm_start_range}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "model = GradientBoostingClassifier(random_state = 702)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "tuned_model = RandomizedSearchCV(estimator           = model,\n",
    "                                 param_distributions = param_grid,\n",
    "                                 cv                  = 5,\n",
    "                                 n_iter              = 500,\n",
    "                                 random_state        = 702,\n",
    "                                 scoring             = make_scorer(roc_auc_score,\n",
    "                                                                   needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_model.fit(x_data, y_data)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_model.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_model.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>d) Complete the code to check the best estimator for the model.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# checking the best estimator for the model\n",
    "tuned_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>e) Manually input the optimal set of hyperparameters when instantiating the model object.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING with best_estimator\n",
    "model = _____\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = \"Tuned Gradient Boosted Machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING with best_estimator\n",
    "model = GradientBoostingClassifier(loss          = 'exponential',\n",
    "                                   learning_rate = 0.1,\n",
    "                                   max_depth     = 2,\n",
    "                                   criterion     = 'squared_error', \n",
    "                                   n_estimators  = 1100,\n",
    "                                   warm_start    = False,\n",
    "                                   random_state  = 702)\n",
    "\n",
    "\n",
    "# using the classification_summary function\n",
    "classification_summary(x          = x_data,\n",
    "                       y          = y_data,\n",
    "                       model      = model,\n",
    "                       model_name = \"Tuned Gradient Boosted Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<br>\n",
    "\n",
    "~~~\n",
    "  _____              ____     ____     _____     \n",
    " |\" ___|    ___   U |  _\"\\ u / __\"| u |_ \" _|    \n",
    "U| |_  u   |_\"_|   \\| |_) |/<\\___ \\/    | |      \n",
    "\\|  _|/     | |     |  _ <   u___) |   /| |\\     \n",
    " |_|      U/| |\\u   |_| \\_\\  |____/>> u |_|U     \n",
    " )(\\\\,-.-,_|___|_,-.//   \\\\_  )(  (__)_// \\\\_    \n",
    "(__)(_/ \\_)-' '-(_/(__)  (__)(__)    (__) (__)   \n",
    "   ____   _         _      ____    ____     _    \n",
    "U /\"___| |\"|    U  /\"\\  u / __\"| u/ __\"| uU|\"|u  \n",
    "\\| | u U | | u   \\/ _ \\/ <\\___ \\/<\\___ \\/ \\| |/  \n",
    " | |/__ \\| |/__  / ___ \\  u___) | u___) |  |_|   \n",
    "  \\____| |_____|/_/   \\_\\ |____/>>|____/>> (_)   \n",
    " _// \\\\  //  \\\\  \\\\    >>  )(  (__))(  (__)|||_  \n",
    "(__)(__)(_\")(\"_)(__)  (__)(__)    (__)    (__)_) \n",
    "                                       \n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    " <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
